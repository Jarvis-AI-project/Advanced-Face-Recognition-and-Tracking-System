{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import datasets, layers, models\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from time import sleep\n",
    "import cv2\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture video from webcam and make dataset\n",
    "class MakeDataset():\n",
    "    def __init__(self):\n",
    "        import cv2\n",
    "        import os\n",
    "        \n",
    "        self.video = cv2.VideoCapture(0)\n",
    "        self.count = 0\n",
    "        self.person_name = input(\"Enter person name: \")\n",
    "        # self.num_images = int(input(\"Enter number of images: \"))\n",
    "        self.path = \"data/\" + self.person_name\n",
    "        if not os.path.exists(self.path):\n",
    "            os.makedirs(self.path)\n",
    "    def  make(self):\n",
    "        while True:\n",
    "            _, frame = self.video.read()\n",
    "            cv2.imshow(\"frame\", frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('s'):\n",
    "                self.count += 1\n",
    "                cv2.imwrite(self.path + \"/\" + self.person_name + '_' + str(self.count) + \".jpg\", frame)\n",
    "                print(f'Image saved: {self.count}', end='\\r')\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "            if self.count == 50:\n",
    "                break\n",
    "    def __final__(self):\n",
    "        self.video.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "# MakeDataset().make()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Listing images in the dataset directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Devasheesh's faces\n",
    "# deva_faces = './data/devasheesh/'\n",
    "# deva_faces = os.listdir(deva_faces)\n",
    "# deva_faces = [deva_faces[i] for i in range(len(deva_faces)) if deva_faces[i].endswith('.jpg')]\n",
    "\n",
    "# # Swarnim's faces\n",
    "# swar_faces = './data/swarnim/'\n",
    "# swar_faces = os.listdir(swar_faces)\n",
    "# swar_faces = [swar_faces[i] for i in range(len(swar_faces)) if swar_faces[i].endswith('.jpg')]\n",
    "\n",
    "# # Negative faces\n",
    "# neg_faces = './data/negative-faces/'\n",
    "# neg_faces = os.listdir(neg_faces)\n",
    "# neg_faces = [neg_faces[i] for i in range(len(neg_faces)) if neg_faces[i].endswith('.jpg')]\n",
    "\n",
    "# Making a function to list all the images in a folder\n",
    "def list_images(path):\n",
    "    image_files = []\n",
    "    for root, _, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_files.append(os.path.join(root, file))\n",
    "    image_files.sort()\n",
    "    return image_files\n",
    "\n",
    "deva_faces = list_images('./data/Devasheesh/')\n",
    "swar_faces = list_images('./data/Swarnim/')\n",
    "neg_faces = list_images('./data/negative-faces/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Making Train and Test Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract the face from the image\n",
    "# using the Haar Cascade Classifier\n",
    "def extract_frontal_face_harr(image_ndarray, grayscale=True, size=(150, 150)):\n",
    "    # Load the cascade\n",
    "    face_cascade = cv2.CascadeClassifier('./haarcascade_frontalface_default.xml')\n",
    "    # Convert into grayscale\n",
    "    if grayscale:\n",
    "        image_gray = cv2.cvtColor(image_ndarray, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        image_gray = image_ndarray\n",
    "    # Detect faces\n",
    "    faces_cord = face_cascade.detectMultiScale(image_gray, 1.3, 5)\n",
    "    # Return the face or None if not found\n",
    "    if len(faces_cord) == 0:\n",
    "        return None, None\n",
    "    # Extract the face\n",
    "    (x, y, w, h) = faces_cord[0]\n",
    "    # Resize the image to 150x150\n",
    "    image_gray_resized = cv2.resize(image_gray[y:y+w, x:x+h], size)\n",
    "    # Return only the face part of the image\n",
    "    return image_gray_resized, faces_cord\n",
    "\n",
    "# using DNN\n",
    "modelFile = \"dnn/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "configFile = \"dnn/deploy.prototxt.txt\"\n",
    "net = cv2.dnn.readNetFromCaffe(configFile, modelFile)\n",
    "\n",
    "def extract_frontal_face(image_ndarray, grayscale=False, size=(150, 150)):\n",
    "    # resize the image to 300x300 for the DNN model\n",
    "    h, w = 300, 300\n",
    "    image_ndarray = cv2.resize(image_ndarray, (h, w))\n",
    "    # Convert into blob\n",
    "    blob = cv2.dnn.blobFromImage(image_ndarray, 1.0, (h, w), (104.0, 177.0, 123.0))\n",
    "    # Convert into grayscale\n",
    "    if grayscale:\n",
    "        image_ndarray = cv2.cvtColor(image_ndarray, cv2.COLOR_BGR2GRAY)\n",
    "    # Detect faces\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > 0.5:\n",
    "            # Get the coordinates of the bounding box\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (x, y, x2, y2) = box.astype(\"int\")\n",
    "\n",
    "            # Extract the face ROI (region of interest) from the image\n",
    "            face = image_ndarray[y:y2, x:x2]\n",
    "            return cv2.resize(face, size), (x, y, x2, y2)\n",
    "        else:\n",
    "            return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to make the image size uniform and make train and test object with labels\n",
    "class dataframe():\n",
    "    def __init__(self):\n",
    "        self.deva_faces = deva_faces\n",
    "        self.swar_faces = swar_faces\n",
    "        self.neg_faces = neg_faces\n",
    "        self.images, self.labels = list(), list()\n",
    "\n",
    "        self.total_images = len(deva_faces)+len(swar_faces)+len(neg_faces)\n",
    "        self.labels_list = ['Unknown', 'Devasheesh', 'Swarnim']\n",
    "\n",
    "        print('Total images: ', self.total_images)\n",
    "\n",
    "        self.datagen = ImageDataGenerator(\n",
    "            rotation_range=40,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "            height_shift_range=0.2, # randomly shift images vertically (fraction of total height)\n",
    "            shear_range=0.2,    # set range for random shear\n",
    "            zoom_range=0.2,    # set range for random zoom\n",
    "            horizontal_flip=True,   # randomly flip images\n",
    "            fill_mode='nearest' # set mode for filling points outside the input boundaries\n",
    "        )\n",
    "\n",
    "    def append_dataframe(self, images_list, label, augment=False, grayscale=False):\n",
    "        count = 1\n",
    "        for image_adr in images_list:\n",
    "            # read the image\n",
    "            image = cv2.imread(image_adr)\n",
    "            # extract the face\n",
    "            face, _ = extract_frontal_face(image, grayscale=grayscale, size=(300, 300))\n",
    "\n",
    "            print('Count: ', count, ' | Image address: ', image_adr, ' | Label: ',self.labels_list[label], ' | Image shape: ', image.shape)\n",
    "            if face is not None:\n",
    "                # print('Face type: ', type(face))\n",
    "                # print('Image shape: ', image.shape)\n",
    "                # print('Face shape: ', face.shape)\n",
    "                # print('Face: ', plt.imshow(face))\n",
    "\n",
    "                # append the face image to the list\n",
    "                if augment:\n",
    "                    # using the image data generator to augment the images\n",
    "                    i = 0\n",
    "                    for batch in self.datagen.flow(face.reshape(1, 300, 300, 3), batch_size=1):\n",
    "                        # append the image to the list\n",
    "                        self.images.append(batch[0])\n",
    "                        self.labels.append(label)\n",
    "                        i += 1\n",
    "                        if i > 10:\n",
    "                            break\n",
    "                    count += 1\n",
    "\n",
    "                else:\n",
    "                    self.images.append(face)\n",
    "                    self.labels.append(label)\n",
    "                    count += 1\n",
    "            else:\n",
    "                print(f'Face not found in {image_adr}')\n",
    "                continue\n",
    "    \n",
    "    def make_dataframe(self):\n",
    "        self.append_dataframe(self.neg_faces, 0, augment=False)\n",
    "        self.append_dataframe(self.deva_faces, 1, augment=True)\n",
    "        self.append_dataframe(self.swar_faces, 2, augment=True)\n",
    "        self.images = np.array(self.images)\n",
    "        self.labels = np.array(self.labels)\n",
    "        print('Images shape: ', self.images.shape)\n",
    "        print('Labels shape: ', self.labels.shape)\n",
    "        return self.images/255.0, self.labels, self.labels_list\n",
    "\n",
    "x = dataframe()\n",
    "images, labels, labels_list = x.make_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in range(len(images)):\n",
    "#     try:\n",
    "#         plt.imshow(images[x])\n",
    "#         plt.title(labels_list[int(labels[x])])\n",
    "#         plt.show()\n",
    "#     except:\n",
    "#         print(f'Error in {x}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(images, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating The CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(300,300,3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(len(labels_list), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_images, train_labels, epochs=2, \n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "print(test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('face_recognition_model_ep2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('face_recognition_model_ep2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, labels_list, grayscale=False):\n",
    "    # initialize the video capture object\n",
    "\n",
    "    # read the frame from the webcam\n",
    "    # frame = cv2.imread('data\\Devasheesh\\Devasheesh_32.jpg')\n",
    "    while True:\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        ret, frame = cap.read()\n",
    "        frame = cv2.resize(frame, (300, 300))\n",
    "        # extract the face\n",
    "        face, (x, y, x2, y2) = extract_frontal_face(\n",
    "            frame, grayscale=grayscale, size=(300, 300))\n",
    "        if face is not None:\n",
    "            # predict the face\n",
    "            pred = model.predict(face.reshape(1, 300, 300, 3))\n",
    "            # get the label\n",
    "            label = labels_list[np.argmax(pred)]\n",
    "            # draw the rectangle and put the text\n",
    "            cv2.rectangle(frame, (x, y), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, label, (x, y-10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "\n",
    "        # show the frame\n",
    "        cv2.imshow('Face Recognition', frame)\n",
    "\n",
    "# call the predict function\n",
    "predict(model, labels_list, grayscale=False)\n",
    "\n",
    "# destroy all windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n",
      "No face detected\n",
      "(480, 640, 3)\n",
      "No face detected\n",
      "(480, 640, 3)\n",
      "No face detected\n",
      "(480, 640, 3)\n",
      "No face detected\n",
      "(480, 640, 3)\n",
      "No face detected\n",
      "(480, 640, 3)\n",
      "No face detected\n",
      "(480, 640, 3)\n",
      "No face detected\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "(480, 640, 3)\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "(480, 640, 3)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Test the model - Self independent test\n",
    "\n",
    "# imports\n",
    "import cv2\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "# load the model\n",
    "model = keras.models.load_model(r\"C:\\models\\face_recognition_model_ep2.h5\")\n",
    "# using DNN\n",
    "modelFile = \"dnn/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "configFile = \"dnn/deploy.prototxt.txt\"\n",
    "net = cv2.dnn.readNetFromCaffe(configFile, modelFile)\n",
    "\n",
    "\n",
    "def extract_frontal_face(image_ndarray, size, grayscale=False):\n",
    "    # resize the image to 300x300 for the DNN model\n",
    "    h, w = 300, 300\n",
    "    image_ndarray = cv2.resize(image_ndarray, (h, w))\n",
    "    # Convert into blob\n",
    "    blob = cv2.dnn.blobFromImage(\n",
    "        image_ndarray, 1.0, (h, w), (104.0, 177.0, 123.0))\n",
    "    # Convert into grayscale\n",
    "    if grayscale:\n",
    "        image_ndarray = cv2.cvtColor(image_ndarray, cv2.COLOR_BGR2GRAY)\n",
    "    # Detect faces\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > 0.5:\n",
    "            # Get the coordinates of the bounding box\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (x, y, x2, y2) = box.astype(\"int\")\n",
    "\n",
    "            # Extract the face ROI (region of interest) from the image\n",
    "            face = image_ndarray[y:y2, x:x2]\n",
    "            return cv2.resize(face, size), (x, y, x2, y2)\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "\n",
    "def predict_video(model, labels_list):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        print(frame.shape)\n",
    "        frame = cv2.resize(frame, (300, 300))\n",
    "        try:\n",
    "            # extract the face\n",
    "            face, (x, y, x2, y2) = extract_frontal_face(\n",
    "                frame, size=(300, 300))\n",
    "            if face is not None:\n",
    "                # predict the face\n",
    "                pred = model.predict(face.reshape(1, 300, 300, 3))\n",
    "                # get the label\n",
    "                label = labels_list[np.argmax(pred)]\n",
    "                # draw the rectangle and put the text\n",
    "                cv2.rectangle(frame, (x, y), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, label, (x, y-10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "\n",
    "            # show the frame\n",
    "            cv2.imshow('Face Recognition', frame)\n",
    "        except:\n",
    "            print(\"No face detected\")\n",
    "\n",
    "\n",
    "# call the predict function\n",
    "predict_video(model, labels_list=['Unknown', 'Devasheesh', 'Swarnim'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7de77cee769da20084e89adc64b8486256d2572f1950a2eedb38473f6ffa9e9f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
