{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import datasets, layers, models\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from time import sleep\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "sess = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initalize logging\n",
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', datefmt='%d-%b-%y %H:%M:%S', filename='main_log_v1.log', filemode='w')\n",
    "logging.basicConfig(level=logging.ERROR, \n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', \n",
    "                    datefmt='%d-%b-%y %H:%M:%S', \n",
    "                    filename='main_log_v1.log', \n",
    "                    filemode='w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "   print(\"Please install GPU version of TF\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU and enable memory growth\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        # print(len(gpus), \"Physical GPUs:\", len(logical_gpus), \"Logical GPU\")\n",
    "        print(f'Physical GPUs: {gpus} Logical GPUs: {logical_gpus}')\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        print('Error: ', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture video from webcam and make dataset\n",
    "class MakeDataset():\n",
    "    def __init__(self):\n",
    "        import cv2\n",
    "        import os\n",
    "        \n",
    "        self.video = cv2.VideoCapture(0)\n",
    "        self.count = 0\n",
    "        self.person_name = input(\"Enter person name: \")\n",
    "        # self.num_images = int(input(\"Enter number of images: \"))\n",
    "        self.path = \"data/\" + self.person_name\n",
    "        if not os.path.exists(self.path):\n",
    "            os.makedirs(self.path)\n",
    "    def make(self):\n",
    "        while True:\n",
    "            _, frame = self.video.read()\n",
    "            cv2.imshow(\"frame\", frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('s'):\n",
    "                self.count += 1\n",
    "                cv2.imwrite(self.path + \"/\" + self.person_name + '_' + str(self.count) + \".jpg\", frame)\n",
    "                print(f'Image saved: {self.count}', end='\\r')\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "            if self.count == 50:\n",
    "                break\n",
    "    def __final__(self):\n",
    "        self.video.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "# MakeDataset().make()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Listing images in the dataset directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Devasheesh's faces\n",
    "# deva_faces = './data/devasheesh/'\n",
    "# deva_faces = os.listdir(deva_faces)\n",
    "# deva_faces = [deva_faces[i] for i in range(len(deva_faces)) if deva_faces[i].endswith('.jpg')]\n",
    "\n",
    "# # Swarnim's faces\n",
    "# swar_faces = './data/swarnim/'\n",
    "# swar_faces = os.listdir(swar_faces)\n",
    "# swar_faces = [swar_faces[i] for i in range(len(swar_faces)) if swar_faces[i].endswith('.jpg')]\n",
    "\n",
    "# # Negative faces\n",
    "# neg_faces = './data/negative-faces/'\n",
    "# neg_faces = os.listdir(neg_faces)\n",
    "# neg_faces = [neg_faces[i] for i in range(len(neg_faces)) if neg_faces[i].endswith('.jpg')]\n",
    "\n",
    "# Making a function to list all the images in a folder\n",
    "def list_images(path):\n",
    "    image_files = []\n",
    "    for root, _, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_files.append(os.path.join(root, file))\n",
    "    image_files.sort()\n",
    "    return image_files\n",
    "\n",
    "deva_faces = list_images('./data/Devasheesh/')\n",
    "swar_faces = list_images('./data/Swarnim/')\n",
    "neg_faces = list_images('./data/negative-faces/')\n",
    "# neg_faces = list_images(r'Y:\\DATASETS\\negative-faces\\VGG-Face2\\data\\test')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Making Train and Test Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract the face from the image\n",
    "# using the Haar Cascade Classifier\n",
    "def extract_frontal_face_harr(image_ndarray, grayscale=True, size=(150, 150)):\n",
    "    # Load the cascade\n",
    "    face_cascade = cv2.CascadeClassifier('./haarcascade_frontalface_default.xml')\n",
    "    # Convert into grayscale\n",
    "    if grayscale:\n",
    "        image_gray = cv2.cvtColor(image_ndarray, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        image_gray = image_ndarray\n",
    "    # Detect faces\n",
    "    faces_cord = face_cascade.detectMultiScale(image_gray, 1.3, 5)\n",
    "    # Return the face or None if not found\n",
    "    if len(faces_cord) == 0:\n",
    "        return None, None\n",
    "    # Extract the face\n",
    "    (x, y, w, h) = faces_cord[0]\n",
    "    # Resize the image to 150x150\n",
    "    image_gray_resized = cv2.resize(image_gray[y:y+w, x:x+h], size)\n",
    "    # Return only the face part of the image\n",
    "    return image_gray_resized, faces_cord\n",
    "\n",
    "# using DNN\n",
    "modelFile = \"dnn/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "configFile = \"dnn/deploy.prototxt.txt\"\n",
    "net = cv2.dnn.readNetFromCaffe(configFile, modelFile)\n",
    "\n",
    "def extract_frontal_face(image_ndarray, grayscale=False, size=(150, 150)):\n",
    "    # resize the image to 300x300 for the DNN model\n",
    "    h, w = 300, 300\n",
    "    image_ndarray = cv2.resize(image_ndarray, (h, w))\n",
    "    # Convert into blob\n",
    "    blob = cv2.dnn.blobFromImage(image_ndarray, 1.0, (h, w), (104.0, 177.0, 123.0))\n",
    "    # Convert into grayscale\n",
    "    if grayscale:\n",
    "        image_ndarray = cv2.cvtColor(image_ndarray, cv2.COLOR_BGR2GRAY)\n",
    "    # Detect faces\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > 0.5:\n",
    "            # Get the coordinates of the bounding box\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (x, y, x2, y2) = box.astype(\"int\")\n",
    "\n",
    "            # Extract the face ROI (region of interest) from the image\n",
    "            face = image_ndarray[y:y2, x:x2]\n",
    "            return cv2.resize(face, size), (x, y, x2, y2)\n",
    "        else:\n",
    "            return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to make the image size uniform and make train and test object with labels\n",
    "class dataframe():\n",
    "    def __init__(self):\n",
    "        self.deva_faces = deva_faces\n",
    "        self.swar_faces = swar_faces\n",
    "        self.neg_faces = neg_faces\n",
    "        self.images, self.labels = list(), list()\n",
    "\n",
    "        self.total_images = len(deva_faces)+len(swar_faces)+len(neg_faces)\n",
    "        self.labels_list = ['Unknown', 'Devasheesh', 'Swarnim']\n",
    "\n",
    "        print('Total images: ', self.total_images)\n",
    "\n",
    "        self.datagen = ImageDataGenerator(\n",
    "            rotation_range=40,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "            height_shift_range=0.2, # randomly shift images vertically (fraction of total height)\n",
    "            shear_range=0.2,    # set range for random shear\n",
    "            zoom_range=0.2,    # set range for random zoom\n",
    "            horizontal_flip=True,   # randomly flip images\n",
    "            fill_mode='nearest' # set mode for filling points outside the input boundaries\n",
    "        )\n",
    "\n",
    "    def append_dataframe(self, images_list, label, augment=False, grayscale=False):\n",
    "        count = 1\n",
    "        for image_adr in images_list:\n",
    "            try:\n",
    "                # read the image\n",
    "                image = cv2.imread(image_adr)\n",
    "                # extract the face\n",
    "                face, _ = extract_frontal_face(image, grayscale=grayscale, size=(300, 300))\n",
    "\n",
    "                print('Count: ', count, ' | Image address: ', image_adr, ' | Label: ',self.labels_list[label], ' | Image shape: ', image.shape)\n",
    "                if face is not None:\n",
    "                    # print('Face type: ', type(face))\n",
    "                    # print('Image shape: ', image.shape)\n",
    "                    # print('Face shape: ', face.shape)\n",
    "                    # print('Face: ', plt.imshow(face))\n",
    "\n",
    "                    # append the face image to the list\n",
    "                    if augment:\n",
    "                        # using the image data generator to augment the images\n",
    "                        i = 0\n",
    "                        for batch in self.datagen.flow(face.reshape(1, 300, 300, 3), batch_size=1):\n",
    "                            # append the image to the list\n",
    "                            self.images.append(batch[0])\n",
    "                            self.labels.append(label)\n",
    "                            i += 1\n",
    "                            if i > 10:\n",
    "                                break\n",
    "                        count += 1\n",
    "                \n",
    "                    else:\n",
    "                        self.images.append(face)\n",
    "                        self.labels.append(label)\n",
    "                        count += 1\n",
    "                else:\n",
    "                    print(f'Face not found in {image_adr}')\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(f'Error: {e} in {image_adr}')\n",
    "                # logging\n",
    "                logging.error(f'Error: {e} in {image_adr}')\n",
    "                continue\n",
    "             \n",
    "    def make_dataframe(self):\n",
    "        self.append_dataframe(self.neg_faces, 0, augment=False)\n",
    "        self.append_dataframe(self.deva_faces, 1, augment=True)\n",
    "        self.append_dataframe(self.swar_faces, 2, augment=True)\n",
    "        self.images = np.array(self.images)\n",
    "        self.labels = np.array(self.labels)\n",
    "        print('Images shape: ', self.images.shape)\n",
    "        print('Labels shape: ', self.labels.shape)\n",
    "        return self.images/255.0, self.labels, self.labels_list\n",
    "\n",
    "x = dataframe()\n",
    "images, labels, labels_list = x.make_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving numpy arrays\n",
    "np.save('images_v1.npy', images)\n",
    "np.save('labels_v1.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading numpy arrays\n",
    "labels_list = ['Unknown', 'Devasheesh', 'Swarnim']\n",
    "images = np.load('images.npy')\n",
    "labels = np.load('labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(700,705):\n",
    "    try:\n",
    "        plt.imshow(images[x])\n",
    "        plt.title(labels_list[int(labels[x])])\n",
    "        plt.show()\n",
    "    except:\n",
    "        print(f'Error in {x}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(images, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating The CNN Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default working architecture (model_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=(300, 300, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "# model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "# model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(len(labels_list), activation='softmax'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v2: Adding more layers with more filters and proper structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(\n",
    "    32, (3, 3), padding='same', input_shape=(300,300,3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.Conv2D(\n",
    "    128, (3, 3), padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.Conv2D(\n",
    "    64, (3, 3), padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.Conv2D(\n",
    "    32, (3, 3), padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(1024, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.7))\n",
    "model.add(layers.Dense(len(labels_list), activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_5 (Conv2D)           (None, 300, 300, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 150, 150, 32)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 150, 150, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 75, 75, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 75, 75, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 37, 37, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 37, 37, 64)        73792     \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 18, 18, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 18, 18, 32)        18464     \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 9, 9, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2592)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1024)              2655232   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 3075      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,843,811\n",
      "Trainable params: 2,843,811\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_images.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current physical devices\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Print the current device\n",
    "    print(\"Currently using GPU:\", gpus[0])\n",
    "else:\n",
    "    print(\"No GPU available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_images, train_labels,\n",
    "                    epochs=15,\n",
    "                    validation_data=(test_images, test_labels),\n",
    "                    batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "print('Accuracy:',test_acc)\n",
    "print('Loss',test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('trained_models/face_recognition_model_v2_ep15_bs16.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutdown system\n",
    "# os.system('shutdown /s /t 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('face_recognition_model_ep2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, labels_list, grayscale=False):\n",
    "    # initialize the video capture object\n",
    "\n",
    "    # read the frame from the webcam\n",
    "    # frame = cv2.imread('data\\Devasheesh\\Devasheesh_32.jpg')\n",
    "    while True:\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        ret, frame = cap.read()\n",
    "        frame = cv2.resize(frame, (300, 300))\n",
    "        # extract the face\n",
    "        face, (x, y, x2, y2) = extract_frontal_face(\n",
    "            frame, grayscale=grayscale, size=(300, 300))\n",
    "        if face is not None:\n",
    "            # predict the face\n",
    "            pred = model.predict(face.reshape(1, 300, 300, 3))\n",
    "            # get the label\n",
    "            label = labels_list[np.argmax(pred)]\n",
    "            # draw the rectangle and put the text\n",
    "            cv2.rectangle(frame, (x, y), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, label, (x, y-10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "\n",
    "        # show the frame\n",
    "        cv2.imshow('Face Recognition', frame)\n",
    "\n",
    "# call the predict function\n",
    "predict(model, labels_list, grayscale=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f54b67cc2b0b5f35f9caf1b9f04c98fdda10ba61be4ae2a9f95103155e744fae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
