{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-05 13:34:48.167402: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-05 13:34:48.278728: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-05 13:34:48.278755: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-05 13:34:48.807498: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-05 13:34:48.807562: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-05 13:34:48.807567: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers, models\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "# from time import sleep\n",
    "import cv2\n",
    "# import pandas as pd\n",
    "# import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 15:53:19.220986: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-04 15:53:19.307875: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/jarvis/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-03-04 15:53:19.308343: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-04 15:53:19.308735: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (training-server): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "sess = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initalize logging\n",
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', datefmt='%d-%b-%y %H:%M:%S', filename='main_log_v1.log', filemode='w')\n",
    "# logging.basicConfig(level=logging.ERROR, \n",
    "#                     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', \n",
    "#                     datefmt='%d-%b-%y %H:%M:%S', \n",
    "#                     filename='main_log_v1.log', \n",
    "#                     filemode='w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install GPU version of TF\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "   print(\"Please install GPU version of TF\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPUs found\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU and enable memory growth\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        # print(len(gpus), \"Physical GPUs:\", len(logical_gpus), \"Logical GPU\")\n",
    "        print(f'Physical GPUs: {gpus} Logical GPUs: {logical_gpus}')\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        print('Error: ', e)\n",
    "else:\n",
    "    print('No GPUs found')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture video from webcam and make dataset\n",
    "class MakeDataset():\n",
    "    def __init__(self):\n",
    "        import cv2\n",
    "        import os\n",
    "        \n",
    "        self.video = cv2.VideoCapture(0)\n",
    "        self.count = 0\n",
    "        self.person_name = input(\"Enter person name: \")\n",
    "        # self.num_images = int(input(\"Enter number of images: \"))\n",
    "        self.path = \"data/\" + self.person_name\n",
    "        if not os.path.exists(self.path):\n",
    "            os.makedirs(self.path)\n",
    "    def make(self):\n",
    "        while True:\n",
    "            _, frame = self.video.read()\n",
    "            cv2.imshow(\"frame\", frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('s'):\n",
    "                self.count += 1\n",
    "                cv2.imwrite(self.path + \"/\" + self.person_name + '_' + str(self.count) + \".jpg\", frame)\n",
    "                print(f'Image saved: {self.count}', end='\\r')\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "            if self.count == 50:\n",
    "                break\n",
    "    def __final__(self):\n",
    "        self.video.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "# MakeDataset().make()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Listing images in the dataset directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Devasheesh's faces\n",
    "# deva_faces = './data/devasheesh/'\n",
    "# deva_faces = os.listdir(deva_faces)\n",
    "# deva_faces = [deva_faces[i] for i in range(len(deva_faces)) if deva_faces[i].endswith('.jpg')]\n",
    "\n",
    "# # Swarnim's faces\n",
    "# swar_faces = './data/swarnim/'\n",
    "# swar_faces = os.listdir(swar_faces)\n",
    "# swar_faces = [swar_faces[i] for i in range(len(swar_faces)) if swar_faces[i].endswith('.jpg')]\n",
    "\n",
    "# # Negative faces\n",
    "# neg_faces = './data/negative-faces/'\n",
    "# neg_faces = os.listdir(neg_faces)\n",
    "# neg_faces = [neg_faces[i] for i in range(len(neg_faces)) if neg_faces[i].endswith('.jpg')]\n",
    "\n",
    "# Making a function to list all the images in a folder\n",
    "def list_images(path):\n",
    "    image_files = []\n",
    "    count = 0\n",
    "    for root, _, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_files.append(os.path.join(root, file))\n",
    "                print(f'Image found: {count}', end='\\r')\n",
    "                count += 1\n",
    "    image_files.sort()\n",
    "    return image_files\n",
    "neg_faces = list_images('/coding-drive/DATASETS/negative-faces/VGG-Face2/data/test/')\n",
    "deva_faces = list_images('/coding-drive/DATASETS/positive_face_datase/Devasheesh/')\n",
    "swar_faces = list_images('/coding-drive/DATASETS/positive_face_datase/Swarnim/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "neg_faces = list_images('/coding-drive/DATASETS/negative-faces/VGG-Face2/data/test/')\n",
    "# neg_faces = list_images(r'Y:\\DATASETS\\negative-faces\\VGG-Face2\\data\\test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deva_faces = list_images('/coding-drive/DATASETS/positive_face_datase/Devasheesh/')\n",
    "# deva_faces = list_images(r'Y:\\DATASETS\\positive_face_dataset\\Devasheesh')\n",
    "# deva_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "swar_faces = list_images('/coding-drive/DATASETS/positive_face_datase/Swarnim/')\n",
    "# swar_faces = list_images(r'Y:\\DATASETS\\positive_face_dataset\\Swarnim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Making Train and Test Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to extract the face from the image\n",
    "# using the Haar Cascade Classifier\n",
    "def extract_frontal_face_harr(image_ndarray, grayscale=True, size=(150, 150)):\n",
    "    # Load the cascade\n",
    "    face_cascade = cv2.CascadeClassifier('./haarcascade_frontalface_default.xml')\n",
    "    # Convert into grayscale\n",
    "    if grayscale:\n",
    "        image_gray = cv2.cvtColor(image_ndarray, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        image_gray = image_ndarray\n",
    "    # Detect faces\n",
    "    faces_cord = face_cascade.detectMultiScale(image_gray, 1.3, 5)\n",
    "    # Return the face or None if not found\n",
    "    if len(faces_cord) == 0:\n",
    "        return None, None\n",
    "    # Extract the face\n",
    "    (x, y, w, h) = faces_cord[0]\n",
    "    # Resize the image to 150x150\n",
    "    image_gray_resized = cv2.resize(image_gray[y:y+w, x:x+h], size)\n",
    "    # Return only the face part of the image\n",
    "    return image_gray_resized, faces_cord\n",
    "\n",
    "# using DNN\n",
    "modelFile = \"dnn/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "configFile = \"dnn/deploy.prototxt.txt\"\n",
    "net = cv2.dnn.readNetFromCaffe(configFile, modelFile)\n",
    "\n",
    "def extract_frontal_face(image_ndarray, grayscale=False, size=(300, 300)):\n",
    "    # resize the image to 300x300 for the DNN model\n",
    "    h, w = 300, 300\n",
    "    image_ndarray = cv2.resize(image_ndarray, (h, w))\n",
    "    # Convert into blob\n",
    "    blob = cv2.dnn.blobFromImage(image_ndarray, 1.0, (h, w), (104.0, 177.0, 123.0))\n",
    "    # Convert into grayscale\n",
    "    if grayscale:\n",
    "        image_ndarray = cv2.cvtColor(image_ndarray, cv2.COLOR_BGR2GRAY)\n",
    "    # Detect faces\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > 0.5:\n",
    "            # Get the coordinates of the bounding box\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (x, y, x2, y2) = box.astype(\"int\")\n",
    "\n",
    "            # Extract the face ROI (region of interest) from the image\n",
    "            face = image_ndarray[y:y2, x:x2]\n",
    "            return cv2.resize(face, size), (x, y, x2, y2)\n",
    "        else:\n",
    "            return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to make the image size uniform and make train and test object with labels\n",
    "class dataframe():\n",
    "    def __init__(self):\n",
    "        self.deva_faces = deva_faces\n",
    "        self.swar_faces = swar_faces\n",
    "        self.neg_faces = neg_faces\n",
    "        self.images, self.labels = list(), list()\n",
    "\n",
    "        self.total_images = len(deva_faces)+len(swar_faces)+len(neg_faces)\n",
    "        self.labels_list = ['Unknown', 'Devasheesh', 'Swarnim']\n",
    "\n",
    "        print('Total images: ', self.total_images)\n",
    "\n",
    "        self.datagen = ImageDataGenerator(\n",
    "            rotation_range=40,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "            height_shift_range=0.2, # randomly shift images vertically (fraction of total height)\n",
    "            shear_range=0.2,    # set range for random shear\n",
    "            zoom_range=0.2,    # set range for random zoom\n",
    "            horizontal_flip=True,   # randomly flip images\n",
    "            fill_mode='nearest' # set mode for filling points outside the input boundaries\n",
    "        )\n",
    "\n",
    "    def append_dataframe(self, images_list, label, augment=False, grayscale=False):\n",
    "        count = 0\n",
    "        for image_adr in images_list:\n",
    "            try:\n",
    "                # read the image\n",
    "                image = cv2.imread(image_adr)\n",
    "                # extract the face\n",
    "                face, _ = extract_frontal_face(image, grayscale=grayscale, size=(360, 480))\n",
    "\n",
    "                print('Count: ', count, end='\\r')\n",
    "                if face is not None:\n",
    "                    # print('Face type: ', type(face))\n",
    "                    # print('Image shape: ', image.shape)\n",
    "                    # print('Face shape: ', face.shape)\n",
    "                    # print('Face: ', plt.imshow(face))\n",
    "\n",
    "                    # append the face image to the list\n",
    "                    if augment:\n",
    "                        # using the image data generator to augment the images\n",
    "                        i = 0\n",
    "                        for batch in self.datagen.flow(face.reshape(1, 300, 300, 3), batch_size=1):\n",
    "                            # append the image to the list\n",
    "                            self.images.append(batch[0])\n",
    "                            self.labels.append(label)\n",
    "                            i += 1\n",
    "                            if i > 10:\n",
    "                                break\n",
    "                        count += 1\n",
    "                \n",
    "                    else:\n",
    "                        self.images.append(face)\n",
    "                        self.labels.append(label)\n",
    "                        count += 1\n",
    "                else:\n",
    "                    print(f'Face not found in {image_adr}')\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(f'Error: {e} in {image_adr}')\n",
    "                # logging\n",
    "                # logging.error(f'Error: {e} in {image_adr}')\n",
    "                continue\n",
    "             \n",
    "    def make_dataframe(self):\n",
    "        self.append_dataframe(self.neg_faces, 0, augment=False)\n",
    "        self.append_dataframe(self.deva_faces, 1, augment=True)\n",
    "        self.append_dataframe(self.swar_faces, 2, augment=True)\n",
    "        self.images = np.array(self.images)\n",
    "        self.labels = np.array(self.labels)\n",
    "        print('Images shape: ', self.images.shape)\n",
    "        print('Labels shape: ', self.labels.shape)\n",
    "        return self.images/255.0, self.labels, self.labels_list\n",
    "\n",
    "x = dataframe()\n",
    "images, labels, labels_list = x.make_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving numpy arrays\n",
    "# np.save('images_v1.npy', images)\n",
    "# np.save('labels_v1.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading numpy arrays\n",
    "# labels_list = ['Unknown', 'Devasheesh', 'Swarnim']\n",
    "# images = np.load('arrays/v1_pos_neg/images.npy')\n",
    "# labels = np.load('arrays/v1_pos_neg/labels.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for x in range(700,705):\n",
    "    try:\n",
    "        plt.imshow(images[x])\n",
    "        plt.title(labels_list[int(labels[x])])\n",
    "        plt.show()\n",
    "    except:\n",
    "        print(f'Error in {x}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split the data into train and test\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(images, labels, test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating The CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the current physical devices\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Print the current device\n",
    "    print(\"Currently using GPU:\", gpus[0])\n",
    "else:\n",
    "    print(\"No GPU available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default working architecture (model_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_v1 = models.Sequential()\n",
    "model_v1.add(layers.Conv2D(\n",
    "    64, (3, 3), activation='relu', input_shape=(360, 480, 3)))\n",
    "model_v1.add(layers.MaxPooling2D((2, 2)))\n",
    "model_v1.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model_v1.add(layers.MaxPooling2D((2, 2)))\n",
    "model_v1.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model_v1.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "model_v1.add(layers.Flatten())\n",
    "# model.add(layers.Dropout(0.5))\n",
    "model_v1.add(layers.Dense(1024, activation='relu'))\n",
    "model_v1.add(layers.Dense(512, activation='relu'))\n",
    "model_v1.add(layers.Dense(256, activation='relu'))\n",
    "model_v1.add(layers.Dense(128, activation='relu'))\n",
    "model_v1.add(layers.Dense(64, activation='relu'))\n",
    "model_v1.add(layers.Dense(32, activation='relu'))\n",
    "model_v1.add(layers.Dense(len(labels_list), activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_v1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_v1.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model_v1.fit(train_images, train_labels,\n",
    "                    epochs=20,\n",
    "                    validation_data=(test_images, test_labels),\n",
    "                    batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model_v1.evaluate(test_images,  test_labels, verbose=2)\n",
    "print('Accuracy:',test_acc)\n",
    "print('Loss',test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v1.save(\n",
    "    './trained_models/face_recognition_model_v1_default_ep20_bs16.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v2: Adding more layers with more filters and proper structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG_FACE_16_layers(input_shape, labels):\n",
    "    model_vgg16 = tf.keras.models.Sequential()\n",
    "\n",
    "    # block 1\n",
    "    model_vgg16.add(tf.keras.layers.Conv2D(64, (3, 3), padding='same',\n",
    "              activation='relu', name='conv1_1', input_shape=input_shape))\n",
    "    model_vgg16.add(tf.keras.layers.Conv2D(\n",
    "        64, (3, 3), padding='same', activation='relu', name='conv1_2'))\n",
    "    model_vgg16.add(tf.keras.layers.MaxPooling2D(\n",
    "        (2, 2), strides=(2, 2), name='pool1'))\n",
    "\n",
    "    # block 2\n",
    "    model_vgg16.add(tf.keras.layers.Conv2D(\n",
    "        128, (3, 3), padding='same', activation='relu', name='conv2_1'))\n",
    "    model_vgg16.add(tf.keras.layers.Conv2D(\n",
    "        128, (3, 3), padding='same', activation='relu', name='conv2_2'))\n",
    "    model_vgg16.add(tf.keras.layers.MaxPooling2D(\n",
    "        (2, 2), strides=(2, 2), name='pool2'))\n",
    "\n",
    "    # block 3\n",
    "    model_vgg16.add(tf.keras.layers.Conv2D(\n",
    "        256, (3, 3), padding='same', activation='relu', name='conv3_1'))\n",
    "    model_vgg16.add(tf.keras.layers.Conv2D(\n",
    "        256, (3, 3), padding='same', activation='relu', name='conv3_2'))\n",
    "    model_vgg16.add(tf.keras.layers.Conv2D(\n",
    "        256, (3, 3), padding='same', activation='relu', name='conv3_3'))\n",
    "    model_vgg16.add(tf.keras.layers.MaxPooling2D(\n",
    "        (2, 2), strides=(2, 2), name='pool3'))\n",
    "\n",
    "    # block 4\n",
    "    model_vgg16.add(tf.keras.layers.Conv2D(\n",
    "        512, (3, 3), padding='same', activation='relu', name='conv4_1'))\n",
    "    model_vgg16.add(tf.keras.layers.Conv2D(\n",
    "        512, (3, 3), padding='same', activation='relu', name='conv4_2'))\n",
    "    model_vgg16.add(tf.keras.layers.Conv2D(\n",
    "        512, (3, 3), padding='same', activation='relu', name='conv4_3'))\n",
    "    model_vgg16.add(tf.keras.layers.MaxPooling2D(\n",
    "        (2, 2), strides=(2, 2), name='pool4'))\n",
    "\n",
    "    # block 5\n",
    "    model_vgg16.add(tf.keras.layers.Conv2D(\n",
    "        512, (3, 3), padding='same', activation='relu', name='conv5_1'))\n",
    "    model_vgg16.add(tf.keras.layers.Conv2D(\n",
    "        512, (3, 3), padding='same', activation='relu', name='conv5_2'))\n",
    "    model_vgg16.add(tf.keras.layers.Conv2D(\n",
    "        512, (3, 3), padding='same', activation='relu', name='conv5_3'))\n",
    "    model_vgg16.add(tf.keras.layers.MaxPooling2D(\n",
    "        (2, 2), strides=(2, 2), name='pool5'))\n",
    "\n",
    "    # flatten and fully connected layers\n",
    "    model_vgg16.add(tf.keras.layers.Flatten(name='flatten'))\n",
    "    model_vgg16.add(tf.keras.layers.Dense(4096, activation='relu', name='fc6'))\n",
    "    model_vgg16.add(tf.keras.layers.Dense(4096, activation='relu', name='fc7'))\n",
    "    model_vgg16.add(tf.keras.layers.Dense(2622, activation='relu', name='fc8'))\n",
    "    model_vgg16.add(tf.keras.layers.Dense(labels, activation='softmax'))\n",
    "\n",
    "    return model_vgg16\n",
    "\n",
    "\n",
    "model_vgg16 = VGG_FACE_16_layers((300, 300, 3), len(labels_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg16.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg16.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_vgg16.fit(train_images, train_labels,\n",
    "                    epochs=10,\n",
    "                    validation_data=(test_images, test_labels),\n",
    "                    batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model_vgg16.evaluate(\n",
    "    test_images,  test_labels, verbose=2)\n",
    "print('Accuracy:',test_acc)\n",
    "print('Loss',test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg16.save('./trained_models/face_recognition_model_vgg16_ep10_bs16.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('face_recognition_model_ep2.h5')\n",
    "# def predict(model, labels_list, grayscale=False):\n",
    "#     # initialize the video capture object\n",
    "\n",
    "#     # read the frame from the webcam\n",
    "#     # frame = cv2.imread('data\\Devasheesh\\Devasheesh_32.jpg')\n",
    "#     while True:\n",
    "#         cap = cv2.VideoCapture(0)\n",
    "#         ret, frame = cap.read()\n",
    "#         frame = cv2.resize(frame, (300, 300))\n",
    "#         # extract the face\n",
    "#         face, (x, y, x2, y2) = extract_frontal_face(\n",
    "#             frame, grayscale=grayscale, size=(300, 300))\n",
    "#         if face is not None:\n",
    "#             # predict the face\n",
    "#             pred = model.predict(face.reshape(1, 300, 300, 3))\n",
    "#             # get the label\n",
    "#             label = labels_list[np.argmax(pred)]\n",
    "#             # draw the rectangle and put the text\n",
    "#             cv2.rectangle(frame, (x, y), (x2, y2), (0, 255, 0), 2)\n",
    "#             cv2.putText(frame, label, (x, y-10),\n",
    "#                         cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "\n",
    "#         # show the frame\n",
    "#         cv2.imshow('Face Recognition', frame)\n",
    "\n",
    "# # call the predict function\n",
    "# predict(model, labels_list, grayscale=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
